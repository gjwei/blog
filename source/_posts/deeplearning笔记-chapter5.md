---
title: deeplearning笔记-chapter5
date: 2018-05-08 21:35:13
tags: deeplearning笔记
---
# 机器学习基础

机器学习：对于某类任务T，和性能度量P，一个计算机课程可以认为从经验E中学习是指，经过经验E改进之后，它在renwuT上由性能度量P衡量的性能有所提升。

<!-- more -->

## 任务T
* 分类
* 回归
* 机器翻译
* 结构化输出
    * 输出值是向量或者其他形式的包含多个值的数据结构
    * 输出值之间内部紧密相关。
* 。。。

## 策略
### 期望损失
模型的期望损失：理论上，模型关于联合分布P(X,Y)的平均意义下的损失。

但是，期望损失是不可求的，因为联合分布不知道。
### 经验风险
对于给定的数据集，模型关于训练数据集的平均损失函数

期望风险是模型关于联合分布的期望风险，经验风险是模型关于**训练数据**的平均损失。

### 经验风险最小化和结构风险最小化
当模型容量很小的时候，采用经验风险最小化（训练误差）作为学习策略会产生过拟合。

结构风险最小化就是为了防止过拟合而提出的策略。

结构风险最小化等价于正则化。

## 经验E
主要在介绍监督学习和无监督学习。

## 示例：线性回归
定义输出为：
$$ y=w^Tx $$
## 模型容量，过拟合，欠拟合
泛化能力：模型在未知的数据上表现良好的能力。


我们会对训练数据和测试数据进行独立同分布的假设，即是说，每个数据集中的样本都是独立同分布的，训练集和测试集数据是**同分布**的。

**模型容量**：表示模型拟合各种函数的能力。容量低的模型很难拟合数据，容量高的模型容易发生过拟合。

一种控制模型容量的方法是选择**假设空间**，即算法可以选择的解决方案的函数集。

### 奥卡姆剃刀理论
原则上，同样可以解释已知观测数据的假设中，我们选择最简单的哪一个。

**统计学习理论告诉我们：**训练误差和测试误差之间的差异的上界会随着模型容量增加而增加；随着数据样本的增多而减小。

### 没有免费午餐理论
在所有的可能的数据生成分布上平均之后，每个分类算法在未事先观测的点上都有相同的错误率。

这意味着，机器学习的研究并不是找一个通用的学习算法或者绝对优秀的算法，而是根据具体的任务，去找到最适合的算法，解决任务。

## 正则化
没有免费午餐理论告诉我们，没有最优的学习算法，特别是没有最优的正则化方法。

## 超参数和验证集
超参数不是通过学习得到的，而是由设计者决定的。

验证集的目的是为了得到最优的超参数。

## 交叉验证

## 估计、偏差和方差
从频率学派角度看，我们假设真是参数$\theta$固定但是未知。而点估计是数据的函数。由于数据是随机采样出来的，数据的任何函数的都是随机的，因此点估计量是个随机变量

### 偏差
偏差定义为：点估计的平均期望和真实参数的差。
$$ bias(\theta_m) = E(\theta_m) - \theta $$

如果偏差等于0，表示无偏估计

### 方差
期望的变化程度是多少。

### 二者关系
偏差-方差分解是解释学习算法泛化性能的一种重要工具。

泛化的误差可以分解为偏差、方差和噪音的和。
$$ E(f;D)=bias^2(x) + var(x) +e $$

偏差度量了学习算法的预测期望和真实结果的偏离程度，即是**刻画了学习算法的拟合能力**，

而方差度量了**数据扰动对模型性能的影响**；

噪声则是学习算法能到到的误差的下限，**即是刻画学习问题本身的难度**

偏差和方差是一种trade-off的关系，在学习过程中，学习训练不足时候，模型拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时是偏差住到了泛化误差。

当模型的训练程度加深，模型拟合能力增强，数据中的扰动能够被学习器学到，方差就会逐渐主导泛化能力错误率。

刚开始阶段，偏差大，方差小，此时是模型是处于欠拟合状态。
之后，随着学习程度加深，会出现偏差小，方差大的情况，此时是处于过拟合状态。

## 最大似然估计
解释MLP的观点是将它看做最小化训练集上的经验分布$p_{data}$和模型分布之间的差异。二者之间的差异可以通过KL散度衡量。

我们知道KL散度是模型预测结果的交叉熵减去真实分布的熵，而后者是固定不变的。

所以，极小化KL散度，等价于极小化交叉熵，也是等价于极小化负对数似然函数。

### 条件对数似然函数和均方误差
对于线性回归问题，我们是假设噪音数据的分布是在$wx$这条直线作为中心的两边正态分布的。

我们就可以定义为：
$$ p(y|x)=N(y;\hat{y}(x; w), \sigma^2) $$
于是便可以得到添加对数似然函数。
$$ NLL = -m log \sigma - \frac{m}{2}log(2\pi) - \sum_{i=1}^m \frac{||\hat{y}^{(i)} - y^{(i)}||^2}{2\sigma^2} $$

可以看到，最后一项就是MSE

### 最大似然性质
1. 当样本数目趋向无穷大时候，收敛率而言是最好的渐进估计。
2. 似然估计具有一致性，以为则当训练数据趋向无穷大时，参数的最大似然估计会收敛到参数的真实分布。

当样本数目小到会发生过拟合的时候，正则化策略可以用来获取训练数据有限情况下方差较小的最大似然有偏版本。

## 贝叶斯估计
频率派估计视角是真实参数是未知的定值，点估计$\hat{\theta}$是考虑数据集上的函数的随机变量。

贝叶斯角度完全不同，他是用概率反映知识状态的确定性程度。数据集能够被观察到，所以不是随机的，真实参数是未知的，所以可以表示为随机变量。

在观测到数据之前，我们将参数已有的知识表示成先验概率分布$p(\theta)$

## 监督学习算法

### 概率监督学习算法
大多数的监督学习算法都是基于估计概率分布$p(y|x)$
得到后验概率之后，可以通过最大化对数似然估计搜索最优解。

使用梯度下降等方法。

### 支持向量机
支持向量机只输出类别，不输出概率。

支持向量机最强大的内容是核技巧。

核函数可以使得学习是隐式的在特征空间中进行，不需要显式地定义特征空间和映射函数。

高斯核函数：
$$ K(u, v) = exp(- \frac{||u-v||^2}{2 \sigma^2}) $$

他会沿着v中从u向外辐射的方向减小。

我们可以将其看成一种模板匹配，训练标签y相关的训练样本x变成了类别y的模板，当测试点$x^{'}$到x距离很小的时候，对应高斯核响应很大，表示二者很相似。

模型进而会对其赋予输入
label y较大的权重。

### 其他监督算法
1. k-最近邻法
2. 决策树算法
## 无监督学习算法
无监督学习算法的学习任务是找到数据的“最佳表示”。
常见的三种包括低维表示、稀疏表示和独立表示。

低维表示是将数据尽可能压缩到一个较小的表示中。

稀疏表示是将数据集嵌入到输入项大多数为0的表示中

独立表示试图分开数据分布中变化的来源，表示的维度统计独立。

### 主成分分析

### k-means聚类算法

### 随机梯度下降
后面有详细去讲，在这里就不多说了。

## 促使深度学习发展的挑战
### 维度灾难问题
### 局部不变性和平滑正则化

为了更好地泛化，机器学习需要由先验信念引导应该学习什么样的函数。

最广泛的隐式“先验”是**平滑先验**或者**局部不变性先验**。这个先验表明学习的函数不应该在小的区域内发生很大的变化

这也是L2正则的作用，限制参数大小，防止参数过大导致分类曲面不够“平滑”

深度学习是假设数据由**因素和特征组合**产生，这些因素和特征可能来自一个层级结构的多个层级。

深度学习的分布式表达带来的指数增益有效解决了维度灾难的问题。

### 流行学习
**流形假设**：现实生活中的图像，文本，声音的概率分布都是高度集中的；另外，每个样本都是被其他高度相似的样本包围，这些高度相似的样本可以通过变换来遍历流形得到。

