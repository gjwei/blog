---
title: 最大似然估计
date: 2018-05-07 20:23:53
tags: deeplearning
---

# 最大似然估计
最大似然估计就是找到一组参数，使得“事情发生的概率最大”

最大似然估计可以转化成对数形式的最大似然估计。而对数似然估计函数其实就是交叉熵！
<!-- more -->
我们知道，交叉熵用来表示衡量模型预测的分布和真实分布的差异，所以，极大化对数似然估计，等价于极小化交叉熵。也就是极小化模型预测和真实分布的差异。

最大似然估计可以看做是最小化训练数据集上的经验分布p_data和模型分布之间的差异，两者之间的差异可以通过KL散度度量。

## 熵，交叉熵，KL散度
参考链接：https://www.zhihu.com/question/41252833

### 熵
信息量：是用来表示事件A的信息量，也叫做编码长度。
熵用来表示多个事件的平均信息量
$$ H(p)=\sum p(i) * log(\frac{1}{p(i)}) $$

### 交叉熵
模型分布q来自真实数据分布p的平均编码长度。
$$ H(p, q)=\sum p(i) * log(\frac{1}{q(i)}) $$

### KL散度
用来衡量模型预测的分布q和真是分布的平均编码长度的差异
$$ D(p||q)=H(p,q) - H(p) $$


## 为什么使用交叉熵(对数似然估计）作为损失函数？
1. 因为数据的真是分布样本在数据给定的时候就已经确定（频率学派的观点），所以，真实分布的熵是一个定值。
此时，KL散度和交叉熵是等价的关系。极小化交叉熵，等价于极小化KL散度，使得模型预测的分布接近样本真实的分布。
2. 激活函数使用使用sigmoid或者softmax函数时，指数函数会出现在输入值很大的情况下，梯度非常小的情况，这会导致梯度信息无法传递。

而交叉熵中的log函数可以抵消掉指数函数的影响。