---
title: deeplearning笔记-chapter11-实践方法论
date: 2018-05-30 20:24:14
tags: deeplearning笔记
---
在机器学习的日常开发中，实践者需要决定是够收集足够的数据，增加或者减少模型容量、增加或者删除正则化项、改进模型的优化、改进模型的近似推断或者调试欧兴的软件实现等。
<!-- more -->
## 性能度量
精度，查准率（precision）
召回率，查全率 Recall

F1 Score = 2 / (1 / p + 1 / R)

## 默认的基准模型

如全连接网络，ReLU，PReLU等激活函数，Dropout， Adam算法，批标准化

## 决定是够收集更多的数据
怎么判断是否要收集更多的数据？
如果模型在训练数据上都没有办法表现良好，一种可能是模型有问题，改进模型。另外一种可能就是数据的质量。

如果模型在训练数据上表现良好，但是在测试数据上表现很差（过拟合），收集更多的数据是解决方法之一（最好的方法），但是代价很大。
这个时候，可以尝试调整超参数，加入正则化策略，BN等）

## 选择超参数
### 手动选择

手动选择参数的主要目的是调整模型的有效容量以匹配任务的复杂性。

对于某些超参数，当数值越大的时候，会发生过拟合。例如中间层隐藏单元的数量，增加数量能够提高模型的容量，容易发生过拟合。
有些超参数太小，也会发生过拟合，如最小权值衰减系数为0，此时学习算法具有最大的容量，反而容易发生过拟合。

学习率可能是最重要的超参数了，相比其他超参数，它以一种非常复杂的方式控制模型的复杂度。当学习了适合问题时候，模型的有效容量最高。
如果超参数过大或者过小，都会对训练产生影响。

![](https://raw.githubusercontent.com/gjwei/images/master/20180530212442.png)

### 网格搜索
网格搜索选择所有超参数的组合进行训练，然后选择出最好的超参数。
问题：计算量随着超参数的数量呈指数增加

### 随机搜索
过程如下，
首先，为每个超参数确定一个边缘分布，例如伯努利分布，或者对数尺度上的均匀分布。例如
log_learningrate ~ U(a, b)

我们**不需要离散化超参数的值**，允许在更大的参数空间上进行搜索，而不产生额外的计算代价。

## 调试优化
1. 可视化计算中模型的行为
2. 可视化最严重的错误
3. 拟合极小的数据集
4. 监控激活函数和梯度的直方图