---
title: deeplearning笔记-chapter10-序列建模：循环和递归网络
date: 2018-05-30 19:59:55
tags: deeplearning笔记
---
CNN和RNN都是NLP中常用的模型，两个模型捕捉特征的角度也不太一样，**CNN善于捕捉文本中关键的局部信息**，而RNN则**善于捕捉文本的上下文信息（考虑语序信息），并且有一定的记忆能力**，两者都可以用在文本分类任务中，而且效果都不错。

RNN的提出主要是为了解决序列问题的长期依赖问题。

<!-- more -->

问题：梯度爆炸和消失
原因：
$$ h^{t} = tanh(W_x X + W_h h^{t-1}) $$
$$ \frac{\partial h^t}{\partial h^{t-1}} = (1 - tanh^2)* W_h $$

所以，根据BPTT，如果梯度从t层传到l层，需要累乘(l - t)次参数值。

如果W的特征值有大于1或者小于1的情况下，会发生梯度爆炸或者消失。

## LSTM
LSTM解决了RNN中常见的梯度消失问题

推导过程如下：
$$ C^t = f_t * C^{t - 1} + i_t * \hat{C^t} $$
所以，
$$ \frac{\partial C^t}{\partial C^{t - 1}} = f_t + ... $$
...表示的值可以忽略不考虑。

$f_t$是模型自动控制的数值，所以，能够解决梯度消失的问题。


