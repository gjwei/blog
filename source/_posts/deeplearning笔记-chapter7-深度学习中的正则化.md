---
title: deeplearning笔记-chapter7-深度学习中的正则化
date: 2018-05-12 15:50:51
tags: deeplearning笔记
---
正则化可以定义为：对学习算法进行修改，目的是减少泛化误差而不是训练误差。

## 参数范数惩罚
![](https://ws1.sinaimg.cn/large/9244e6f1gy1fr8lxp52lwj209u01swec.jpg)
我们通常是只对权重进行惩罚，而不对偏置进行惩罚。精确拟合偏置所需要的数据要少于拟合权重的数据。

正则化偏置可能会导致明显的欠拟合情况。

### L2正则化
权重衰减
![](https://ws1.sinaimg.cn/large/9244e6f1gy1fr8m08otq5j20ar01y0sl.jpg)
权重衰减过程中，每个权值的衰减方向是指向0的方向，而步长是和w大小成正比，所以，权值会朝着0衰减，但是步长会越来越慢，最终达到0的附近。

### L1正则化
![](https://ws1.sinaimg.cn/large/9244e6f1gy1fr8m3o32n3j20lh032t8x.jpg)
L1正则化产生系数解。
可以用作特征选择。

## 作为约束的范数惩罚

通过直接给模型的参数设置越是来进行正则化。

## 数据增强
* 图像进行翻转，平移等
* 为输入层注入噪音

## 噪音鲁棒性
一种方法是添加方差极小的噪音，这等价于对权重事实范数惩罚。

另一种使用方法是将噪音增加到权重。主要是用于RNN网络中。
### 对输入label添加噪音
大多数数据集的label都会有一定的错误。错误的label将会误导系统，所以，可以通过对label增加噪音降低错误label的影响。

**标签平滑**的方法：将分类目标中的0和1分别替换成$\frac{\theta}{k - 1}$和$1 - \theta$

## 多任务学习方法

## Early Stop

提前终止等价于L2正则化

## 参数绑定和参数共享
## Bagging和其他集成方法
通过结合几个模型降低泛化误差。一般采用模型平均的方法。
Bagging方法的原理是，通过平均多个高方差低偏差的模型的结果，可以显著的降低总和结果的方差，而不会改变模型的偏差。

这是方差的公式决定的。

## Dropout
提供一种廉价的bagging集成近似，能够训练和评估指数级数量的神经网络

## 对抗训练
对抗样本产生的原因是过度线性。

