---
title: 深度学习岗位面试问题整理笔记
date: 2018-05-12 23:44:45
tags: deeplearning
---
最近在看知乎，看到有些内容挺好的，摘录一下
原文链接：https://zhuanlan.zhihu.com/p/25005808

https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b

## SGD中的S（stochastic)代表什么
随机啊，还能代表什么？

It is called stochastic because each small set of examples gives a noisy estimate of the average gradient over all examples.

它被称为随机因为每个小例子集都给出了所有例子中平均梯度的有噪音的估计。

<!-- more -->
## CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？

以上几个不相关问题的相关性在于，都存在**局部与整体**的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。如下图：低层次的直线／曲线等特征，组合成为不同的形状，最后得到汽车的表示。

![](https://pic4.zhimg.com/80/v2-8555de443211e31f6e3967fe0fab83b3_hd.jpg)

CNN抓住此共性的手段主要有四个：**局部连接／权值共享／池化操作／多层次结构**。

局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。

## 什么样的资料集不适合深度学习？
* 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。
* 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。

## 对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法?
没有免费的午餐理论。

## 广义线性模型是怎被应用在深度学习中?
深度学习从统计学角度，可以看做递归的广义线性模型。
广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。
深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的链接函数。逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数，很多类似的方法在统计学和神经网络中的名称不一样，容易引起初学者（这里主要指我）的困惑。下图是一个对照表：
![](https://pic2.zhimg.com/80/v2-29d9d42212fd2294e71c2f3e760791d4_hd.jpg)

## 梯度消失问题？
比如，常用的激活函数sigmoid，在进行反向传播的时候，梯度信息往后进行传递，需要乘以激活值对应的导数。我们知道，sigmoid的导数的最大值是0.25，如果经过多层的反向传递，梯度最终会衰减到0。这就是梯度消失。

所以，目前的深度学习主要是 使用Relu作为激活函数，减缓梯度消失的问题。

## Dying ReLUs问题？
如果神经元通过前向传播得到的值小于0，通过ReLU函数的时候得到的结果为0，而且此时对应的导数也是0，梯度信息无法向后传递，参数无法更新。神经元可能会永久“dying"

## RNN中的梯度爆炸
在进行时间方向的反向传播的时候，梯度信息往后传播过程需要累乘当前RNN的神经元参数W。经过T次累乘后，如果参数W的最大特征值大于1，梯度会爆炸；如果小于1，梯度会消失。

## CNN VS DNN VS RNN 
全连接的DNN结构中，下层的神经元和所有上层的神经元都是连接的，带来的问题是参数的膨胀。不仅容易过拟合，而且很容易陷入到局部最优解。

为了解决这个问题，CNN通过卷积核的方法，同一个卷积核在所有图像的输入都是共享参数的，达到了稀疏连接的效果。卷积之后的输出也是能够保留原先图像的局部信息。从而，CNN的参数相比于DNN，大大减少，也可以进行更深的堆叠。

在RNN中，神经元的输出会作为下一个时间步的输入，通过在这种循环的机制实现对序列变长的数据进行运算处理。

## GLOVE
加权的最小二乘目标，直接优化的目标是最小化两个单词向量的点积和其共同出现次数的对数之间的差异：
$$ J=∑_{i,j=1}^{V}f(X_{ij})(w^T_iw_j+bi+b~j−logXij)^2 $$

