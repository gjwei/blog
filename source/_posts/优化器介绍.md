---
title: 优化器介绍
date: 2018-05-03 21:48:41
tags: 
- deep learning
- NLP
---
# 深度学习中的优化器

## AdaGrad 
亦称为适应性梯度（Adaptive Gradient），它允许学习率基于参数进行调整，而不需要在学习过程中人为调整学习率。AdaGrad 对具有较大梯度的参数相应地有一个快速下降的过程，而具有小梯度的参数在学习率上有相对较小的下降速度。因此，AdaGrad 成了稀疏数据如图像识别和 NLP 的天然选择。
<!-- more -->
## RMSProp 算法（Hinton，2012）
修改 AdaGrad 以在非凸情况下表现更好，它改变梯度累积为指数加权的移动平均值，从而丢弃距离较远的历史梯度信息。


## Adam 
adam算法的提出者描述其为两种随机梯度下降扩展式的优点集合，即：
* 适应性梯度算法（AdaGrad）为每一个参数保留一个学习率以提升在稀疏梯度（即自然语言和计算机视觉问题）上的性能。
* 均方根传播（RMSProp）基于权重梯度最近量级的均值为每一个参数适应性地保留学习率。这意味着算法在非稳态和在线问题上有很有优秀的性能。
Adam不仅如RMSProp一样利用了二阶动量信息，而且还使用了一阶的动量信息，并且对得到的动量进行了偏差纠正操作。

问题：限制更新只依赖于少数历史梯度确实会引起显著的收敛性问题。所以，Adam的更新学习速率在神经网络这样的非凸问题上会变化很大，无法收敛。

## Amsgrad
通过将二阶动量限制为历史梯度的最大值，保证了学习速率是单调递减的，从而可以保证收敛
