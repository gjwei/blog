---
title: deeplearning笔记-chapter8-深度模型的优化
date: 2018-05-13 20:27:21
tags: deeplearning笔记
---
# 深度模型的优化

优化的问题：寻找一组参数，能够显著的降低损失函数。损失函数通常包括整个数据集上的性能评估和额外的正则化
<!-- more -->
## 学习和纯粹的优化的不同
深度学习的优化算法和和传统的优话算法有几个方面的不同：
* 在深度学习/机器学习的优化过程中，我们关注某些性能度量P,但是它可能是不可求解的，所以，我们是间接的优化P。我们希望通过降低代价函数 $J(\theta)$来提高P
* 而纯优化是最小化目标J本身。

### 经验风险最小化
最小化平均训练误差的训练过程是经验风险最小化的过程。

### 代理损失函数
我们真正关心的损失函数（比如分类误差）并不能高效的优化（因为不是凸函数）。例如，在二元分类中，我们真正关心的是0-1损失函数，
$$ l(y,y^)=∑_{i=1}^mχ(yi≠y^i) $$
但是，这个损失函数无法进行优化。

这种情况下，我们会选择使用代理损失函数，将原来的优化问题转化为一个近似的优化问题，而且近似的优化问题更容易求解。

提前终止

### 批量算法和小批量算法
1. 假设样本之间独立，那么n个样本的均值的标准差是$\sigma / \sqrt{n}$，其中$\sigma$是样本值的真是方差，分母为$\sqrt{n}$表示更多的样本来估计梯度的方法的回报是小于线性的。

2. 计算量的问题。

所以，我么会采用随机采样少量样本作为一个batch，然后计算平均值。

使用整个训练集的优化算法成为**批量或者确定性**

深度学习采用的是小批量或者**小批量随机**d的方法，通常简称为*随机（stochastic）方法

小批量带下通常由以下因素决定：
* 大批量会得到更加准确的梯度，但是回报是小于线性的
* 极小的批量无法充分利用多核的架构
* 批量大小和内存占用成正比
* 硬件对于特征的数（比如2的指数）的运行时间更小
* 小批量在学习过程中增加了噪音的影响，一定程度上起到了正则化的方法

不同的优化算法从不同的小批量中获取到不同的信息，一般而言，基于梯度的更新g相对鲁棒，在较小的批量就能成功，比如100.

基于Hessian矩阵的二阶更新方法则通常需要更大的批量。

**小批量是随机抽取的这点很重要，因为样本的无偏估计要求样本之间是独立的。**通常的做法是将训练样本shuffle。

## 神经网络中的挑战
## 病态问题
病态体现在随机梯度”卡“在某个位置，此时即使很小的更新步长也大大增加代价函数。

### 局部极小值
对于非凸的问题，如深度学习，有可能存在多个局部极小值。

但是，大多数局部极小值都具有很小的代价函数，我们能不能找到全局最小点并不重要。

### 高原、鞍点和其他平坦区域
对于高纬度非凸函数，局部极小值实际上都远小于另一种梯度为0的点：鞍点。

在鞍点出，Hessian矩阵同时有正负特征值。

Hessian在极小值处只有正特征值。

### 悬崖和梯度爆炸哦
遇到梯度极大的悬崖结构，梯度更新很大将可能导致权值远离最优点

解决方法：梯度截断的方法。

### 长期依赖
主要出现在序列模型中，由于变深的结构使得模型丧失了学习到先前信息的能力，让优化变得困难。

在多个事件步上讲输入重复和权值W相乘，将会导致结果丢弃了x中与权值W的主特征向量正交的成分。

## 基本算法
### 随机梯度下降
按照数据生成分布抽取m个独立同分布的赝本，九三他们的梯度均值，就可以得到梯度的无偏估计

![](https://ws1.sinaimg.cn/large/9244e6f1gy1frarzxnhv8j20pv099acd.jpg)

在实践中，我们有必要随着时间推移逐渐降低学习率。

### 动量

